{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import skimage.io as io\n",
    "from skimage.transform import resize\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "c3d_pre_trained = 'pre_trained_weights/c3d_ucf101.pth'\n",
    "data_dir = \"data\"\n",
    "video_features_file = \"features_video.npz\"\n",
    "pca_video_features_file = \"features_video_pca.npz\"\n",
    "print(device)\n",
    "\n",
    "max_frames_per_clip = 500\n",
    "use_pca = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.cs.utexas.edu/users/ml/clamp/videoDescription/\n",
    "\n",
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DataLoader, self).__init__()\n",
    "        self.pca = None\n",
    "        self.n_components = 512\n",
    "        self.pca = PCA(n_components=self.n_components)\n",
    "        \n",
    "    def create_data_dirs(self):\n",
    "        clips = sorted(glob(join(\"youtube_clips\", \"*.avi\")))\n",
    "        \n",
    "        num_clip = 1\n",
    "        for clip in clips:\n",
    "            print(\"Saving clip - {}/{} - {}\".format(num_clip, len(clips), clip))\n",
    "            \n",
    "            cap = cv2.VideoCapture(clip)\n",
    "            names = clip.split('/')[1].split('.')[0].split('_')\n",
    "            if len(names) > 3:\n",
    "                for j in range(1, len(names) - 2):\n",
    "                    names[0] += \"_\" + names[j]\n",
    "                names[1] = names[-2]\n",
    "                names[2] = names[-1]\n",
    "            \n",
    "            dir_name = \"{}/{}_{}_{}\".format(data_dir, names[0], names[1], names[2])\n",
    "            try:\n",
    "                os.mkdir(dir_name)\n",
    "            except OSError:  \n",
    "                print (\"Creation of the directory {} failed\".format(names))\n",
    "            \n",
    "            i = 1\n",
    "            while(True):\n",
    "                ret, frame = cap.read()\n",
    "                if ret == False:\n",
    "                    break\n",
    "                    \n",
    "                file_name = \"{}/{}.png\".format(dir_name, i)\n",
    "                cv2.imwrite(file_name, frame)\n",
    "                i += 1\n",
    "                \n",
    "            cap.release()\n",
    "            num_clip += 1\n",
    "            \n",
    "    def get_video_clip(self, clip_name, verbose=True):\n",
    "        \"\"\"\n",
    "        Loads a clip to be fed to C3D for classification.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        clip_name: str\n",
    "            the name of the clip (subfolder in 'data').\n",
    "        verbose: bool\n",
    "            if True, shows all the frames (default=True)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            a pytorch batch (num_batch, channels, frames, height, width)\n",
    "        \"\"\"\n",
    "\n",
    "        clip = sorted(glob(join('data', clip_name, '*.png')))\n",
    "        num_frames = len(clip)\n",
    "        if len(clip) > max_frames_per_clip:\n",
    "            num_frames = max_frames_per_clip\n",
    "        \n",
    "        clip = np.array([resize(io.imread(frame), output_shape=(112, 200), preserve_range=True, anti_aliasing=True, \n",
    "                                mode='reflect') for frame in clip])\n",
    "        clip = clip[:num_frames, :, 44:44+112, :]  # crop centrally\n",
    "\n",
    "        if verbose:\n",
    "            clip_img = np.reshape(clip.transpose(1, 0, 2, 3), (112, num_frames * 112, 3))\n",
    "            figure(figsize = (20,2))\n",
    "            io.imshow(clip_img.astype(np.uint8), interpolation='nearest')\n",
    "            io.show()\n",
    "    \n",
    "        clip = clip.transpose(3, 0, 1, 2)  # ch, fr, h, w\n",
    "        clip = np.expand_dims(clip, axis=0)  # batch axis\n",
    "        clip = np.float32(clip)\n",
    "        \n",
    "        return torch.from_numpy(clip).to(device)\n",
    "    \n",
    "    def extract_full_video_features(self):\n",
    "        clips = sorted(glob(join(data_dir, '*')))\n",
    "        feature_dict = {}\n",
    "        \n",
    "        i = 1\n",
    "        for clip in clips:\n",
    "            clip_name = clip.split('/')[-1]\n",
    "            print(\"{}/{} - {}\".format(i, len(clips), clip_name)) \n",
    "            \n",
    "            x = self.get_video_clip(clip_name, False)\n",
    "            features = c3d(x).data.cpu().numpy()\n",
    "            \n",
    "            if clip_name in feature_dict:\n",
    "                feature_dict[clip_name].append(features)\n",
    "            else:\n",
    "                feature_dict[clip_name] = [features]\n",
    "            \n",
    "            del x\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        self.write_features(feature_dict, is_pca=False)\n",
    "    \n",
    "    def extract_pca_video_features(self):\n",
    "        features_dict = np.load(video_features_file)\n",
    "        features = np.zeros((0, 4096))\n",
    "        for key in features_dict:\n",
    "            value = features_dict[key]\n",
    "            features = np.concatenate((features, value), axis=0)\n",
    "        \n",
    "        features = (features - np.min(features, axis=1).reshape(-1,1))/(np.ptp(features, axis=1).reshape(-1,1))\n",
    "        pca_features = self.pca_transform(features)\n",
    "        \n",
    "        pca_features_dict = {}\n",
    "        for key in features_dict:\n",
    "            num_rows = features_dict[key].shape[0]\n",
    "            for i in range(num_rows):\n",
    "                if key in pca_features_dict:\n",
    "                    pca_features_dict[key].append(pca_features[i])\n",
    "                else:\n",
    "                    pca_features_dict[key] = [pca_features[i]]\n",
    "        \n",
    "        self.write_features(pca_features_dict, is_pca=True)\n",
    "\n",
    "    def pca_transform(self, features):\n",
    "        return self.pca.fit_transform(features)\n",
    "        \n",
    "    def write_features(self, features, is_pca):\n",
    "        if is_pca:\n",
    "            np.savez(pca_video_features_file, **features)\n",
    "        else:\n",
    "            np.savez(video_features_file, **features)\n",
    "            \n",
    "    def validate_pca_file(self):\n",
    "        features_dict = np.load(video_features_file)\n",
    "        pca_features_dict = np.load(pca_video_features_file)\n",
    "        assert len(features_dict) == len(pca_features_dict)\n",
    "        \n",
    "        for key in features_dict:\n",
    "            r1, c1 = features_dict[key].shape\n",
    "            r2, c2 = pca_features_dict[key].shape\n",
    "            assert r1 == r2\n",
    "            assert c2 == 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/DavideA/c3d-pytorch\n",
    "\n",
    "class C3D(nn.Module):\n",
    "    def __init__(self, pre_trained=True):\n",
    "        super(C3D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n",
    "\n",
    "        self.fc6 = nn.Linear(8192, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.fc8 = nn.Linear(4096, 487)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        if pre_trained:\n",
    "            self.__load_pretrained_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            h = self.relu(self.conv1(x))\n",
    "            h = self.pool1(h)\n",
    "    \n",
    "            h = self.relu(self.conv2(h))\n",
    "            h = self.pool2(h)\n",
    "    \n",
    "            h = self.relu(self.conv3a(h))\n",
    "            h = self.relu(self.conv3b(h))\n",
    "            h = self.pool3(h)\n",
    "    \n",
    "            h = self.relu(self.conv4a(h))\n",
    "            h = self.relu(self.conv4b(h))\n",
    "            h = self.pool4(h)\n",
    "    \n",
    "            h = self.relu(self.conv5a(h))\n",
    "            h = self.relu(self.conv5b(h))\n",
    "            h = self.pool5(h)\n",
    "    \n",
    "            h = h.view(-1, 8192)\n",
    "            h = self.fc6(h)\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def __load_pretrained_weights(self):\n",
    "        \"\"\"Initialiaze network.\"\"\"\n",
    "        corresp_name = {\n",
    "                        # Conv1\n",
    "                        \"features.0.weight\": \"conv1.weight\",\n",
    "                        \"features.0.bias\": \"conv1.bias\",\n",
    "                        # Conv2\n",
    "                        \"features.3.weight\": \"conv2.weight\",\n",
    "                        \"features.3.bias\": \"conv2.bias\",\n",
    "                        # Conv3a\n",
    "                        \"features.6.weight\": \"conv3a.weight\",\n",
    "                        \"features.6.bias\": \"conv3a.bias\",\n",
    "                        # Conv3b\n",
    "                        \"features.8.weight\": \"conv3b.weight\",\n",
    "                        \"features.8.bias\": \"conv3b.bias\",\n",
    "                        # Conv4a\n",
    "                        \"features.11.weight\": \"conv4a.weight\",\n",
    "                        \"features.11.bias\": \"conv4a.bias\",\n",
    "                        # Conv4b\n",
    "                        \"features.13.weight\": \"conv4b.weight\",\n",
    "                        \"features.13.bias\": \"conv4b.bias\",\n",
    "                        # Conv5a\n",
    "                        \"features.16.weight\": \"conv5a.weight\",\n",
    "                        \"features.16.bias\": \"conv5a.bias\",\n",
    "                         # Conv5b\n",
    "                        \"features.18.weight\": \"conv5b.weight\",\n",
    "                        \"features.18.bias\": \"conv5b.bias\",\n",
    "                        # fc6\n",
    "                        \"classifier.0.weight\": \"fc6.weight\",\n",
    "                        \"classifier.0.bias\": \"fc6.bias\",\n",
    "                        # fc7\n",
    "                        \"classifier.3.weight\": \"fc7.weight\",\n",
    "                        \"classifier.3.bias\": \"fc7.bias\",\n",
    "                        }\n",
    "\n",
    "        p_dict = torch.load(c3d_pre_trained)\n",
    "        s_dict = self.state_dict()\n",
    "        for name in p_dict:\n",
    "            if name not in corresp_name:\n",
    "                continue\n",
    "            s_dict[corresp_name[name]] = p_dict[name]\n",
    "        self.load_state_dict(s_dict)\n",
    "\n",
    "    def __init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c3d = C3D(pre_trained=True)\n",
    "c3d.to(device)\n",
    "c3d.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "# data_loader.extract_full_video_features()\n",
    "# data_loader.extract_pca_video_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
