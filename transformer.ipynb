{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import glob\n",
    "from glob import glob\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# video data directory\n",
    "video_descriptions_csv = \"../video-summarization/video_description.csv\"\n",
    "vocab_file = \"vocab_10_sentence.pickle\"\n",
    "video_descriptions_file = \"video_descriptions_10_sentence.pickle\"\n",
    "video_features_file = \"../video-summarization/features_video_pca.npz\"\n",
    "\n",
    "# Number of training videos\n",
    "num_train_set = 1300\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, train=True):\n",
    "        print(\"Loading Data Loader instance for Train = {}...\".format(train))\n",
    "        \n",
    "        self.device = device\n",
    "        self.video_descriptions_csv = video_descriptions_csv\n",
    "        self.video_descriptions_file = video_descriptions_file\n",
    "        self.vocab_file = vocab_file\n",
    "        self.video_features_file = video_features_file\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # max sentences to include in the dataset. '0' to include all sentences\n",
    "        self.max_sentence_num = 10\n",
    "        self.max_words_num = 25\n",
    "        \n",
    "        self.num_train_set = num_train_set\n",
    "        \n",
    "        print(\"Loading video descriptions...\")\n",
    "        self.video_descriptions = self.load_descriptions()\n",
    "        \n",
    "        print(\"Loading vocabulary...\")\n",
    "        self.vocab = self.load_vocabulary()\n",
    "        \n",
    "        train_test = list(self.video_descriptions.keys())\n",
    "        \n",
    "        if train:\n",
    "            self.names = train_test[:self.num_train_set]\n",
    "        else:\n",
    "            self.names = train_test[self.num_train_set:]\n",
    "        \n",
    "        print(\"Loading video features...\")\n",
    "        self.video_features = np.load(self.video_features_file)\n",
    "        \n",
    "        print(\"Data Loader initialized\")\n",
    "    \n",
    "    def load_descriptions(self):\n",
    "         return pickle.load(open(self.video_descriptions_file, 'rb'))\n",
    "    \n",
    "    def load_vocabulary(self):\n",
    "        return pickle.load(open(self.vocab_file, 'rb'))\n",
    "        \n",
    "    def create_descriptions_from_csv(self):\n",
    "        desc = pd.read_csv(self.video_descriptions_csv)\n",
    "        desc = desc[(desc['Language'] == 'English')]\n",
    "        desc = desc[['VideoID', 'Start', 'End', 'Description']]\n",
    "        desc_dict = {}\n",
    "\n",
    "        for row in desc.iterrows():\n",
    "            key = str(row[1][0]) + '_' + str(row[1][1]) + '_' + str(row[1][2])\n",
    "            if not os.path.exists(\"../video-summarization/data/\" + key):\n",
    "                continue\n",
    "                \n",
    "            if key in desc_dict:\n",
    "                if self.max_sentence_num != 0 and len(desc_dict[key]) < self.max_sentence_num:\n",
    "                    desc_dict[key].append(str(row[1][3]))\n",
    "            else:\n",
    "                desc_dict[key] = [str(row[1][3])]\n",
    "            \n",
    "        return desc_dict\n",
    "    \n",
    "    def create_full_vocab(self):\n",
    "        # load coco vocabulary\n",
    "        # vocab = pickle.load(open(coco_vocab_dir, 'rb'))\n",
    "        vocab = Vocabulary()\n",
    "        vocab.add_word('<pad>')\n",
    "        vocab.add_word('<start>')\n",
    "        vocab.add_word('<end>')\n",
    "        vocab.add_word('<unk>')\n",
    "        \n",
    "        for key in self.video_descriptions:\n",
    "            sentences = ' '.join(self.video_descriptions[key])\n",
    "            \n",
    "            for word in sentences.split(' '):\n",
    "                filtered_word = word.lower().split('.')[0]\n",
    "                if filtered_word not in vocab.word2idx:\n",
    "                    vocab.add_word(filtered_word)\n",
    "                    \n",
    "        return vocab \n",
    "    \n",
    "    def get_words_from_index(self, tensor):\n",
    "        words_list = []\n",
    "        for idx in tensor.data.cpu().numpy():\n",
    "            words_list.append(self.vocab.idx2word[idx])\n",
    "        return words_list\n",
    "    \n",
    "    def get_one_hot_encoded_all(self, video_id):\n",
    "        target = []\n",
    "            \n",
    "        for sentence in self.video_descriptions[video_id]:\n",
    "            x = torch.zeros(len(self.vocab), dtype=torch.float32)\n",
    "            for word in sentence.split(' '):\n",
    "                filtered_word = word.lower().split('.')[0]\n",
    "                x[self.vocab.word2idx[filtered_word]] = 1\n",
    "                target.append(x)\n",
    "            \n",
    "        return torch.stack(target)\n",
    "    \n",
    "    def get_one_hot_encoded(self, video_id):\n",
    "        descriptions = self.video_descriptions[video_id]\n",
    "        index = np.random.randint(low=len(descriptions))\n",
    "        \n",
    "        target = torch.zeros(self.max_words_num, len(self.vocab), dtype=torch.float32)\n",
    "        for word in descriptions[index].split(' '):\n",
    "            filtered_word = word.lower().split('.')[0]\n",
    "            target[i, self.vocab.word2idx[filtered_word]] = 1\n",
    "            \n",
    "        return target\n",
    "    \n",
    "    def data_generator(self):\n",
    "        \n",
    "        while True:\n",
    "            indexes = np.random.choice(a=np.arange(len(self.names)), size=self.batch_size)\n",
    "            max_num_rows = 0\n",
    "            for i in range(len(indexes)):\n",
    "                features = self.video_features[self.names[indexes[i]]]\n",
    "                if features.shape[0] > max_num_rows:\n",
    "                    max_num_rows = features.shape[0]\n",
    "            \n",
    "            x = torch.zeros(self.batch_size, max_num_rows, 512)\n",
    "            y = []\n",
    "            for i in range(len(indexes)):\n",
    "                video_id = self.names[indexes[i]]\n",
    "                features = self.video_features[video_id]\n",
    "                x[i,:features.shape[0],:] = torch.from_numpy(features)\n",
    "                y.append(self.get_one_hot_encoded(video_id))\n",
    "                \n",
    "            yield x.to(device), torch.stack(y).to(device)\n",
    "            \n",
    "    def batch_data_generator(self):\n",
    "        \n",
    "        for i in range(0, len(self.names), self.batch_size):\n",
    "            curr_batch_size = self.batch_size\n",
    "            if (i + self.batch_size) > len(self.names):\n",
    "                curr_batch_size = len(self.names) - i\n",
    "                \n",
    "            indexes = np.random.permutation(np.arange(i, i + curr_batch_size))\n",
    "            max_seq_len = 0\n",
    "            for i in range(len(indexes)):\n",
    "                features = self.video_features[self.names[indexes[i]]]\n",
    "                if features.shape[0] > max_seq_len:\n",
    "                    max_seq_len = features.shape[0]\n",
    "            \n",
    "            x = torch.zeros(curr_batch_size, max_seq_len, 512)\n",
    "            y = []\n",
    "            for i in range(len(indexes)):\n",
    "                video_id = self.names[indexes[i]]\n",
    "                features = self.video_features[video_id]\n",
    "                x[i,:features.shape[0],:] = torch.from_numpy(features)\n",
    "                y.append(self.get_one_hot_encoded(video_id))\n",
    "                \n",
    "            yield x.to(device), torch.stack(y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Loader instance for Train = True...\n",
      "Loading video descriptions...\n",
      "Loading vocabulary...\n",
      "Loading video features...\n",
      "Data Loader initialized\n",
      "Loading Data Loader instance for Train = False...\n",
      "Loading video descriptions...\n",
      "Loading vocabulary...\n",
      "Loading video features...\n",
      "Data Loader initialized\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train=True)\n",
    "test_loader = DataLoader(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "1 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "2 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "3 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "4 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "5 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "6 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "7 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "8 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "9 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "10 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "11 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "12 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "13 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "14 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "15 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "16 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "17 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "18 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "19 torch.Size([64, 31, 512]) torch.Size([64, 25, 5883])\n",
      "20 torch.Size([20, 31, 512]) torch.Size([20, 25, 5883])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(train_loader.batch_data_generator()):\n",
    "    print(i, x.size(), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0.0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "#         nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
