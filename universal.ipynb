{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utransformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utransformer.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as I\n",
    "import numpy as np\n",
    "import math\n",
    "from common_layer import EncoderLayer ,DecoderLayer ,MultiHeadAttention , \\\n",
    "Conv ,PositionwiseFeedForward ,LayerNorm ,_gen_bias_mask ,_gen_timing_signal\n",
    "from Embed import Embedder\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q):\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    PAD = 0\n",
    "    padding_mask = seq_k.eq(PAD)\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "    return padding_mask\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer Encoder module. \n",
    "    Inputs should be in the shape [batch_size, length, hidden_size]\n",
    "    Outputs will have the shape [batch_size, length, hidden_size]\n",
    "    Refer Fig.1 in https://arxiv.org/pdf/1706.03762.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                 filter_size, max_length=49, input_dropout=0.0, layer_dropout=0.0, \n",
    "                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False, act=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            embedding_size: Size of embeddings\n",
    "            hidden_size: Hidden size\n",
    "            num_layers: Total layers in the Encoder\n",
    "            num_heads: Number of attention heads\n",
    "            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n",
    "            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n",
    "            output_depth: Size last dimension of the final output\n",
    "            filter_size: Hidden size of the middle layer in FFN\n",
    "            max_length: Max sequence length (required for timing signal)\n",
    "            input_dropout: Dropout just after embedding\n",
    "            layer_dropout: Dropout for each layer\n",
    "            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n",
    "            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n",
    "            use_mask: Set to True to turn on future value masking\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n",
    "        ## for t\n",
    "        self.position_signal = _gen_timing_signal(num_layers, hidden_size)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.act = act\n",
    "        params =(hidden_size, \n",
    "                 total_key_depth or hidden_size,\n",
    "                 total_value_depth or hidden_size,\n",
    "                 filter_size, \n",
    "                 num_heads, \n",
    "                 _gen_bias_mask(max_length) if use_mask else None,\n",
    "                 layer_dropout, \n",
    "                 attention_dropout, \n",
    "                 relu_dropout)\n",
    "\n",
    "        self.enc = EncoderLayer(*params)\n",
    "        \n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        if(self.act):\n",
    "            self.act_fn = ACT_basic(hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.input_dropout(inputs)\n",
    "\n",
    "        if(self.act):\n",
    "            x, (remainders,n_updates) = self.act_fn(x, inputs, self.enc, self.timing_signal, self.position_signal, self.num_layers)\n",
    "            return x, (remainders,n_updates)\n",
    "        else:\n",
    "            for l in range(self.num_layers):\n",
    "                x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.data)\n",
    "                x += self.position_signal[:, l, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.data)\n",
    "                x = self.enc(x)\n",
    "            return x\n",
    "        \n",
    "        \n",
    "class ACT_basic(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(ACT_basic, self).__init__()\n",
    "        self.sigma = nn.Sigmoid()\n",
    "        self.p = nn.Linear(hidden_size,1)  \n",
    "        self.p.bias.data.fill_(1) \n",
    "        self.threshold = 1 - 0.1\n",
    "\n",
    "    def forward(self, state, inputs, fn, time_enc, pos_enc, max_hop, encoder_output=None):\n",
    "        # init_hdd\n",
    "        ## [B, S]\n",
    "        halting_probability = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n",
    "        ## [B, S\n",
    "        remainders = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n",
    "        ## [B, S]\n",
    "        n_updates = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n",
    "        ## [B, S, HDD]\n",
    "        previous_state = torch.zeros_like(inputs).cuda()\n",
    "        step = 0\n",
    "        # for l in range(self.num_layers):\n",
    "        while( ((halting_probability<self.threshold) & (n_updates < max_hop)).byte().any()):\n",
    "            # Add timing signal\n",
    "            state = state + time_enc[:, :inputs.shape[1], :].type_as(inputs.data)\n",
    "            state = state + pos_enc[:, step, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.data)\n",
    "\n",
    "            p = self.sigma(self.p(state)).squeeze(-1)\n",
    "            # Mask for inputs which have not halted yet\n",
    "            still_running = (halting_probability < 1.0).float()\n",
    "\n",
    "            # Mask of inputs which halted at this step\n",
    "            new_halted = (halting_probability + p * still_running > self.threshold).float() * still_running\n",
    "\n",
    "            # Mask of inputs which haven't halted, and didn't halt this step\n",
    "            still_running = (halting_probability + p * still_running <= self.threshold).float() * still_running\n",
    "\n",
    "            # Add the halting probability for this step to the halting\n",
    "            # probabilities for those input which haven't halted yet\n",
    "            halting_probability = halting_probability + p * still_running\n",
    "\n",
    "            # Compute remainders for the inputs which halted at this step\n",
    "            remainders = remainders + new_halted * (1 - halting_probability)\n",
    "\n",
    "            # Add the remainders to those inputs which halted at this step\n",
    "            halting_probability = halting_probability + new_halted * remainders\n",
    "\n",
    "            # Increment n_updates for all inputs which are still running\n",
    "            n_updates = n_updates + still_running + new_halted\n",
    "\n",
    "            # Compute the weight to be applied to the new state and output\n",
    "            # 0 when the input has already halted\n",
    "            # p when the input hasn't halted yet\n",
    "            # the remainders when it halted this step\n",
    "            update_weights = p * still_running + new_halted * remainders\n",
    "\n",
    "            if(encoder_output):\n",
    "                state, _ = fn((state,encoder_output))\n",
    "            else:\n",
    "                # apply transformation on the state\n",
    "                state = fn(state)\n",
    "\n",
    "            # update running part in the weighted state and keep the rest\n",
    "            previous_state = ((state * update_weights.unsqueeze(-1)) + (previous_state * (1 - update_weights.unsqueeze(-1))))\n",
    "            ## previous_state is actually the new_state at end of hte loop \n",
    "            ## to save a line I assigned to previous_state so in the next \n",
    "            ## iteration is correct. Notice that indeed we return previous_state\n",
    "            step+=1\n",
    "        return previous_state, (remainders,n_updates)\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer Decoder module. \n",
    "    Inputs should be in the shape [batch_size, length, hidden_size]\n",
    "    Outputs will have the shape [batch_size, length, hidden_size]\n",
    "    Refer Fig.1 in https://arxiv.org/pdf/1706.03762.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                 filter_size, max_length=100, input_dropout=0.0, layer_dropout=0.0, \n",
    "                 attention_dropout=0.0, relu_dropout=0.0, act=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            embedding_size: Size of embeddings\n",
    "            hidden_size: Hidden size\n",
    "            num_layers: Total layers in the Encoder\n",
    "            num_heads: Number of attention heads\n",
    "            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n",
    "            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n",
    "            output_depth: Size last dimension of the final output\n",
    "            filter_size: Hidden size of the middle layer in FFN\n",
    "            max_length: Max sequence length (required for timing signal)\n",
    "            input_dropout: Dropout just after embedding\n",
    "            layer_dropout: Dropout for each layer\n",
    "            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n",
    "            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Embed\n",
    "        self.embed = Embedder(vocab_size, total_key_depth)\n",
    "        \n",
    "        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n",
    "        self.position_signal = _gen_timing_signal(num_layers, hidden_size)\n",
    "        self.num_layers = num_layers\n",
    "        self.act = act\n",
    "        params =(hidden_size, \n",
    "                 total_key_depth or hidden_size,\n",
    "                 total_value_depth or hidden_size,\n",
    "                 filter_size, \n",
    "                 num_heads, \n",
    "                 _gen_bias_mask(max_length), # mandatory\n",
    "                 layer_dropout, \n",
    "                 attention_dropout, \n",
    "                 relu_dropout)\n",
    "\n",
    "        self.proj_flag = False\n",
    "        if(embedding_size == hidden_size):\n",
    "            self.embedding_proj = nn.Linear(embedding_size, hidden_size, bias=False)\n",
    "            self.proj_flag = True\n",
    "        self.dec = DecoderLayer(*params) \n",
    "        \n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        if(self.act):\n",
    "            self.act_fn = ACT_basic(hidden_size)\n",
    "    \n",
    "    def forward(self, inputs, encoder_output):\n",
    "        #Add input dropout\n",
    "        x = self.embed(inputs.long()).float()\n",
    "        x = self.input_dropout(x)\n",
    "                \n",
    "        if(self.act):\n",
    "            x, (remainders, n_updates) = self.act_fn(x, inputs, self.dec, self.timing_signal, self.position_signal, self.num_layers, encoder_output)\n",
    "            return x, (remainders,n_updates)\n",
    "        else:\n",
    "            for l in range(self.num_layers):\n",
    "                x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.float().data)\n",
    "                x += self.position_signal[:, l, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.float().data)\n",
    "                x, _ = self.dec((x, encoder_output))\n",
    "        return x\n",
    "    \n",
    "class UTransformer(nn.Module):\n",
    "    def __init__(self, num_vocab, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                 filter_size, max_length=71, input_dropout=0.0, layer_dropout=0.0, \n",
    "                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False, act=False ):\n",
    "        super(UTransformer, self).__init__()\n",
    "        self.embedding_dim = embedding_size\n",
    "        self.emb = nn.Embedding(num_vocab, embedding_size, padding_idx=0)\n",
    "        self.transformer_encoder = Encoder(embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                                filter_size, max_length=71, input_dropout=input_dropout, layer_dropout=layer_dropout, \n",
    "                                attention_dropout=attention_dropout, relu_dropout=relu_dropout, use_mask=False, act=act)\n",
    "\n",
    "        self.transformer_decoder = Decoder(num_vocab, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, \n",
    "                                           total_value_depth, filter_size, max_length=27, input_dropout=0.0, \n",
    "                                           layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0, act=False)\n",
    "        \n",
    "        self.W = nn.Linear(hidden_size, num_vocab)\n",
    "\n",
    "        # Share the weight matrix between target word embedding & the final logit dense layer\n",
    "        # self.W.weight = self.emb.weight\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        ## POSITIONAL MASK\n",
    "        self.mask = nn.Parameter(I.constant_(torch.empty(11, self.embedding_dim), 1))\n",
    "\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        e_outputs = self.transformer_encoder(src)\n",
    "        \n",
    "        d_output = self.transformer_decoder(trg, e_outputs)\n",
    "        output = self.W(d_output)\n",
    "        \n",
    "        self.e_outputs = e_outputs\n",
    "        self.d_output = d_output\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "from Models import get_model\n",
    "import torch.nn.functional as F\n",
    "from Optim import CosineWithRestarts\n",
    "from Batch import create_masks\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle as pickle\n",
    "\n",
    "from DataLoader import DataLoader\n",
    "from Vocabulary import Vocabulary\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_key_pad_mask(seq_k, seq_q):\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    PAD = 0\n",
    "    padding_mask = seq_k.eq(PAD)\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "    return padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer Encoder module. \n",
    "    Inputs should be in the shape [batch_size, length, hidden_size]\n",
    "    Outputs will have the shape [batch_size, length, hidden_size]\n",
    "    Refer Fig.1 in https://arxiv.org/pdf/1706.03762.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                 filter_size, max_length=49, input_dropout=0.0, layer_dropout=0.0, \n",
    "                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False, act=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            embedding_size: Size of embeddings\n",
    "            hidden_size: Hidden size\n",
    "            num_layers: Total layers in the Encoder\n",
    "            num_heads: Number of attention heads\n",
    "            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n",
    "            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n",
    "            output_depth: Size last dimension of the final output\n",
    "            filter_size: Hidden size of the middle layer in FFN\n",
    "            max_length: Max sequence length (required for timing signal)\n",
    "            input_dropout: Dropout just after embedding\n",
    "            layer_dropout: Dropout for each layer\n",
    "            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n",
    "            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n",
    "            use_mask: Set to True to turn on future value masking\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n",
    "        ## for t\n",
    "        self.position_signal = _gen_timing_signal(num_layers, hidden_size)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.act = act\n",
    "        params =(hidden_size, \n",
    "                 total_key_depth or hidden_size,\n",
    "                 total_value_depth or hidden_size,\n",
    "                 filter_size, \n",
    "                 num_heads, \n",
    "                 _gen_bias_mask(max_length) if use_mask else None,\n",
    "                 layer_dropout, \n",
    "                 attention_dropout, \n",
    "                 relu_dropout)\n",
    "\n",
    "        self.proj_flag = False\n",
    "        #if(embedding_size == hidden_size):\n",
    "        #    self.embedding_proj = nn.Linear(embedding_size, hidden_size, bias=False)\n",
    "        #    self.proj_flag = True\n",
    "\n",
    "        self.enc = EncoderLayer(*params)\n",
    "        \n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        if(self.act):\n",
    "            self.act_fn = ACT_basic(hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.input_dropout(inputs)\n",
    "\n",
    "#         if(self.proj_flag):\n",
    "#             x = self.embedding_proj(x)\n",
    "\n",
    "        if(self.act):\n",
    "            x, (remainders,n_updates) = self.act_fn(x, inputs, self.enc, self.timing_signal, self.position_signal, self.num_layers)\n",
    "            return x, (remainders,n_updates)\n",
    "        else:\n",
    "            for l in range(self.num_layers):\n",
    "                x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.data)\n",
    "                x += self.position_signal[:, l, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.data)\n",
    "                x = self.enc(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACT_basic(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(ACT_basic, self).__init__()\n",
    "        self.sigma = nn.Sigmoid()\n",
    "        self.p = nn.Linear(hidden_size,1)  \n",
    "        self.p.bias.data.fill_(1) \n",
    "        self.threshold = 1 - 0.1\n",
    "\n",
    "    def forward(self, state, inputs, fn, time_enc, pos_enc, max_hop, encoder_output=None):\n",
    "        # init_hdd\n",
    "        ## [B, S]\n",
    "        halting_probability = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n",
    "        ## [B, S\n",
    "        remainders = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n",
    "        ## [B, S]\n",
    "        n_updates = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n",
    "        ## [B, S, HDD]\n",
    "        previous_state = torch.zeros_like(inputs).cuda()\n",
    "        step = 0\n",
    "        # for l in range(self.num_layers):\n",
    "        while( ((halting_probability<self.threshold) & (n_updates < max_hop)).byte().any()):\n",
    "            # Add timing signal\n",
    "            state = state + time_enc[:, :inputs.shape[1], :].type_as(inputs.data)\n",
    "            state = state + pos_enc[:, step, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.data)\n",
    "\n",
    "            p = self.sigma(self.p(state)).squeeze(-1)\n",
    "            # Mask for inputs which have not halted yet\n",
    "            still_running = (halting_probability < 1.0).float()\n",
    "\n",
    "            # Mask of inputs which halted at this step\n",
    "            new_halted = (halting_probability + p * still_running > self.threshold).float() * still_running\n",
    "\n",
    "            # Mask of inputs which haven't halted, and didn't halt this step\n",
    "            still_running = (halting_probability + p * still_running <= self.threshold).float() * still_running\n",
    "\n",
    "            # Add the halting probability for this step to the halting\n",
    "            # probabilities for those input which haven't halted yet\n",
    "            halting_probability = halting_probability + p * still_running\n",
    "\n",
    "            # Compute remainders for the inputs which halted at this step\n",
    "            remainders = remainders + new_halted * (1 - halting_probability)\n",
    "\n",
    "            # Add the remainders to those inputs which halted at this step\n",
    "            halting_probability = halting_probability + new_halted * remainders\n",
    "\n",
    "            # Increment n_updates for all inputs which are still running\n",
    "            n_updates = n_updates + still_running + new_halted\n",
    "\n",
    "            # Compute the weight to be applied to the new state and output\n",
    "            # 0 when the input has already halted\n",
    "            # p when the input hasn't halted yet\n",
    "            # the remainders when it halted this step\n",
    "            update_weights = p * still_running + new_halted * remainders\n",
    "\n",
    "            if(encoder_output):\n",
    "                state, _ = fn((state,encoder_output))\n",
    "            else:\n",
    "                # apply transformation on the state\n",
    "                state = fn(state)\n",
    "\n",
    "            # update running part in the weighted state and keep the rest\n",
    "            previous_state = ((state * update_weights.unsqueeze(-1)) + (previous_state * (1 - update_weights.unsqueeze(-1))))\n",
    "            ## previous_state is actually the new_state at end of hte loop \n",
    "            ## to save a line I assigned to previous_state so in the next \n",
    "            ## iteration is correct. Notice that indeed we return previous_state\n",
    "            step+=1\n",
    "        return previous_state, (remainders,n_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer Decoder module. \n",
    "    Inputs should be in the shape [batch_size, length, hidden_size]\n",
    "    Outputs will have the shape [batch_size, length, hidden_size]\n",
    "    Refer Fig.1 in https://arxiv.org/pdf/1706.03762.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                 filter_size, max_length=100, input_dropout=0.0, layer_dropout=0.0, \n",
    "                 attention_dropout=0.0, relu_dropout=0.0, act=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            embedding_size: Size of embeddings\n",
    "            hidden_size: Hidden size\n",
    "            num_layers: Total layers in the Encoder\n",
    "            num_heads: Number of attention heads\n",
    "            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n",
    "            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n",
    "            output_depth: Size last dimension of the final output\n",
    "            filter_size: Hidden size of the middle layer in FFN\n",
    "            max_length: Max sequence length (required for timing signal)\n",
    "            input_dropout: Dropout just after embedding\n",
    "            layer_dropout: Dropout for each layer\n",
    "            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n",
    "            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Embed\n",
    "        self.embed = Embedder(vocab_size, total_key_depth)\n",
    "        \n",
    "        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n",
    "        self.position_signal = _gen_timing_signal(num_layers, hidden_size)\n",
    "        self.num_layers = num_layers\n",
    "        self.act = act\n",
    "        params =(hidden_size, \n",
    "                 total_key_depth or hidden_size,\n",
    "                 total_value_depth or hidden_size,\n",
    "                 filter_size, \n",
    "                 num_heads, \n",
    "                 _gen_bias_mask(max_length), # mandatory\n",
    "                 layer_dropout, \n",
    "                 attention_dropout, \n",
    "                 relu_dropout)\n",
    "\n",
    "        self.proj_flag = False\n",
    "        if(embedding_size == hidden_size):\n",
    "            self.embedding_proj = nn.Linear(embedding_size, hidden_size, bias=False)\n",
    "            self.proj_flag = True\n",
    "        self.dec = DecoderLayer(*params) \n",
    "        \n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        if(self.act):\n",
    "            self.act_fn = ACT_basic(hidden_size)\n",
    "    \n",
    "    def forward(self, inputs, encoder_output):\n",
    "        #Add input dropout\n",
    "        x = self.embed(inputs.long()).float()\n",
    "        x = self.input_dropout(x)\n",
    "                \n",
    "        if(self.act):\n",
    "            x, (remainders, n_updates) = self.act_fn(x, inputs, self.dec, self.timing_signal, self.position_signal, self.num_layers, encoder_output)\n",
    "            return x, (remainders,n_updates)\n",
    "        else:\n",
    "            for l in range(self.num_layers):\n",
    "                x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.float().data)\n",
    "                x += self.position_signal[:, l, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.float().data)\n",
    "                x, _ = self.dec((x, encoder_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTransformer(nn.Module):\n",
    "    def __init__(self, num_vocab, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                 filter_size, max_length=71, input_dropout=0.0, layer_dropout=0.0, \n",
    "                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False, act=False ):\n",
    "        super(UTransformer, self).__init__()\n",
    "        self.embedding_dim = embedding_size\n",
    "        self.emb = nn.Embedding(num_vocab, embedding_size, padding_idx=0)\n",
    "        self.transformer_encoder = Encoder(embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                                filter_size, max_length=71, input_dropout=input_dropout, layer_dropout=layer_dropout, \n",
    "                                attention_dropout=attention_dropout, relu_dropout=relu_dropout, use_mask=False, act=act)\n",
    "\n",
    "        self.transformer_decoder = Decoder(num_vocab, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, \n",
    "                                           total_value_depth, filter_size, max_length=27, input_dropout=0.0, \n",
    "                                           layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0, act=False)\n",
    "        \n",
    "        self.W = nn.Linear(hidden_size, num_vocab)\n",
    "\n",
    "        # Share the weight matrix between target word embedding & the final logit dense layer\n",
    "        # self.W.weight = self.emb.weight\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        ## POSITIONAL MASK\n",
    "        self.mask = nn.Parameter(I.constant_(torch.empty(11, self.embedding_dim), 1))\n",
    "\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        e_outputs = self.transformer_encoder(src)\n",
    "        \n",
    "        d_output = self.transformer_decoder(trg, e_outputs)\n",
    "        output = self.W(d_output)\n",
    "        \n",
    "        self.e_outputs = e_outputs\n",
    "        self.d_output = d_output\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UTransformer(num_vocab=15000, embedding_size=512, hidden_size=512, num_layers=4, num_heads=8, \n",
    "                     total_key_depth=512, total_value_depth=512, filter_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, trainloader):\n",
    "    print(\"training model...\")\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    if opt.checkpoint > 0:\n",
    "        cptime = time.time()\n",
    "    \n",
    "    for epoch in range(opt.epochs):\n",
    "        print(\"epoch: \", epoch)\n",
    "\n",
    "        total_loss = 0\n",
    "        for i, (src, trg, vid_names) in enumerate(trainloader.batch_data_generator()):\n",
    "            trg_input = trg[:, :-1] # not include the end of sentence\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, opt)\n",
    "\n",
    "            preds = model(src, trg_input)\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            opt.optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys)\n",
    "            loss.backward()\n",
    "            opt.optimizer.step()\n",
    "            if opt.SGDR == True: \n",
    "                opt.sched.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "        epoch_time = (time.time() - start)\n",
    "        print(\"%dm %ds: loss = %.3f\\n\" %(epoch_time//60, epoch_time%60, avg_loss))\n",
    "\n",
    "        if opt.checkpoint > 0 and ((time.time()-cptime)//60) // opt.checkpoint >= 1:\n",
    "            torch.save(model.state_dict(), 'weights/model_weights')\n",
    "            cptime = time.time()\n",
    "            print(\"model saved at epoch \", epoch)\n",
    "\n",
    "    # save final weights\n",
    "    torch.save(model.state_dict(), 'weights/model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-no_cuda', action='store_true')\n",
    "parser.add_argument('-SGDR', action='store_true')\n",
    "parser.add_argument('-epochs', type=int, default=100)\n",
    "parser.add_argument('-d_model', type=int, default=512)\n",
    "parser.add_argument('-n_layers', type=int, default=6)\n",
    "parser.add_argument('-heads', type=int, default=8)\n",
    "parser.add_argument('-dropout', type=int, default=0.1)\n",
    "parser.add_argument('-printevery', type=int, default=1)\n",
    "parser.add_argument('-lr', type=int, default=0.0001)\n",
    "parser.add_argument('-load_weights')\n",
    "parser.add_argument('-create_valset', action='store_true')\n",
    "parser.add_argument('-max_strlen', type=int, default=80)\n",
    "parser.add_argument('-floyd', action='store_true')\n",
    "parser.add_argument('-checkpoint', type=int, default=0)\n",
    "parser.add_argument('-batch_size', type=int, default=64)\n",
    "parser.add_argument('-vid_feat_size', type=int, default=512)\n",
    "parser.add_argument('-save_freq', type=int, default=5)\n",
    "# DataLoader\n",
    "parser.add_argument('-num_train_set', type=int, default=1300)\n",
    "parser.add_argument('-video_features_file', default='../data/features_video_rgb_pca_i3d.npz')\n",
    "parser.add_argument('-video_descriptions_file', default='../data/video_descriptions_10_sentence.pickle')\n",
    "parser.add_argument('-vocab_file', default='../data/vocab_10_sentence.pickle')\n",
    "parser.add_argument('-video_descriptions_csv', default='../data/video_description.csv')\n",
    "parser.add_argument('-gpu_id', type=int, default=0)\n",
    "parser.add_argument('-device', default='cuda:0')\n",
    "parser.add_argument('-f')\n",
    "\n",
    "opt = parser.parse_args()\n",
    "\n",
    "opt.device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Loader instance for Train = True...\n",
      "Loading video descriptions...\n",
      "Loading vocabulary...\n",
      "Loading video features...\n",
      "Data Loader initialized\n"
     ]
    }
   ],
   "source": [
    "trainloader = DataLoader(opt=opt, train=True)\n",
    "\n",
    "opt.optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "if opt.SGDR == True:\n",
    "    opt.sched = CosineWithRestarts(opt.optimizer, T_max=opt.train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n",
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:05<00:00,  4.88it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 6s: loss = 6.690\n",
      "\n",
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:04<00:00,  4.67it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 10s: loss = 2.141\n",
      "\n",
      "epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:04<00:00,  4.70it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 14s: loss = 1.784\n",
      "\n",
      "epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:04<00:00,  4.62it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 19s: loss = 1.617\n",
      "\n",
      "epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:04<00:00,  4.71it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 23s: loss = 1.535\n",
      "\n",
      "epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11/21 [00:02<00:02,  4.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-fa7b41526246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-0f526fcf9ef7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, opt, trainloader)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mtrg_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# not include the end of sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/volume/video-summarization/dev2/DataLoader.py\u001b[0m in \u001b[0;36mbatch_data_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mvideo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;31m# y.append(self.get_one_hot_encoded(video_id))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m#   directly into the array memory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0mmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model.to(opt.device)\n",
    "train_model(model, opt, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
