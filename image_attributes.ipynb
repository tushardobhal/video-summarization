{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from os.path import join\n",
    "import glob\n",
    "from glob import glob\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# video data directory\n",
    "data_dir = \"data/\"\n",
    "# ResNet coco pre-trained weights\n",
    "resnet_coco_weights = \"pre_trained_weights/resnet_152_coco.pkl\"\n",
    "# coco data dir\n",
    "coco_vocab_dir = \"coco_data/vocab.pkl\"\n",
    "video_descriptions_file = \"video_description.csv\"\n",
    "attr_net_weights = \"pre_trained_weights/attr_net.pth\"\n",
    "\n",
    "# load every <image_skip_parameter> image\n",
    "max_num_images_per_clip = 32\n",
    "# use n most frequent words\n",
    "num_most_common_words = 10\n",
    "# square image size\n",
    "image_size = 224\n",
    "# Number of training videos\n",
    "num_train_set = 1300\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, train=True):\n",
    "        print(\"Loading Data Loader instance for train = {}\".format(train))\n",
    "        \n",
    "        self.device = device\n",
    "        self.data_path = data_dir\n",
    "        self.video_descriptions_file = video_descriptions_file\n",
    "        self.image_size = image_size\n",
    "        self.max_num_images_per_clip = max_num_images_per_clip\n",
    "        self.num_most_common_words = num_most_common_words\n",
    "        \n",
    "        self.num_train_set = num_train_set\n",
    "        # load names of all the data directories\n",
    "        train_test = sorted(glob(join(self.data_path, \"*\")))\n",
    "        \n",
    "        if train:\n",
    "            self.names = train_test[:self.num_train_set]\n",
    "        else:\n",
    "            self.names = train_test[self.num_train_set:]\n",
    "        \n",
    "        # load all sentences in the dictionary with key as video_id\n",
    "        print(\"Loading sentences for each video into dictionary...\")\n",
    "        self.video_descriptions = self.load_csv()\n",
    "        \n",
    "        # dictionary containing n most frequent words for each video\n",
    "        self.most_freq_words = {}\n",
    "        \n",
    "        # load COCO dictionary and add absent words to vocabulary\n",
    "        print(\"Loading words into vocabulary...\")\n",
    "        self.vocab = self.load_full_vocab()\n",
    "        \n",
    "        self.index = 0\n",
    "\n",
    "        self.data_transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(image_size),\n",
    "                torchvision.transforms.CenterCrop(image_size),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "        print(\"Data Loader initialized\")\n",
    "    \n",
    "    def load_csv(self):\n",
    "        desc = pd.read_csv(video_descriptions_file)\n",
    "        desc = desc[(desc['Language'] == 'English')]\n",
    "        desc = desc[['VideoID', 'Description']]\n",
    "        desc_dict = {}\n",
    "        for row in desc.iterrows():\n",
    "            if row[1][0] in desc_dict:\n",
    "                desc_dict[row[1][0]].append(str(row[1][1]))\n",
    "            else:\n",
    "                desc_dict[row[1][0]] = [str(row[1][1])]\n",
    "        return desc_dict\n",
    "    \n",
    "    def load_full_vocab(self):\n",
    "        stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "        stop_words.add('-')\n",
    "        vocab = pickle.load(open(coco_vocab_dir, 'rb'))\n",
    "        \n",
    "        for key in self.video_descriptions:\n",
    "            sentences = ' '.join(self.video_descriptions[key])\n",
    "            most_freq_words_with_count = nltk.FreqDist(word.lower().split('.')[0] \n",
    "                                    for word in sentences.split(' ') if word.lower() not in stop_words) \n",
    "            \n",
    "            most_freq_words = []\n",
    "            for word in most_freq_words_with_count.most_common(self.num_most_common_words):\n",
    "                most_freq_words.append(word[0])\n",
    "                if word[0] not in vocab.word2idx:\n",
    "                    vocab.add_word(word[0])\n",
    "            \n",
    "            self.most_freq_words[key] = most_freq_words\n",
    "        return vocab\n",
    "    \n",
    "    def image_loader(self, image_name):\n",
    "        \"\"\"load image, returns cuda tensor\"\"\"\n",
    "        image = Image.open(image_name)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = self.data_transforms(image).float()\n",
    "        image = image.unsqueeze(0)\n",
    "        return image[0]\n",
    "\n",
    "    def show(self, img):\n",
    "        npimg = img.cpu().detach().numpy()\n",
    "        npimg = np.transpose(npimg, (1,2,0))\n",
    "        if npimg.shape[2] == 3:\n",
    "            plt.imshow(npimg)\n",
    "        else:\n",
    "            plt.imshow(npimg[:,:,0], cmap='gray')\n",
    "\n",
    "    def imshow(self, img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.cpu().detach().numpy()\n",
    "        plt.figure(figsize=(10,2))\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)), aspect='auto')\n",
    "    \n",
    "    def get_video_id(self, clip_name):\n",
    "        names = clip_name.split('/')[1].split('.')[0].split('_')\n",
    "        if len(names) > 3:\n",
    "            for j in range(1, len(names) - 2):\n",
    "                names[0] += \"_\" + names[j]\n",
    "        return names[0]    \n",
    "            \n",
    "    def get_video_descriptions(self, clip_name):\n",
    "        return self.video_descriptions[self.get_video_id(clip_name)]\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    def get_words_from_index(self, tensor):\n",
    "        words_list = []\n",
    "        for idx in tensor.data.cpu().numpy():\n",
    "            words_list.append(self.vocab.idx2word[idx])\n",
    "        return words_list\n",
    "    \n",
    "    def data_generator(self):\n",
    "\n",
    "        while True:\n",
    "            x = []\n",
    "            unsorted_clip = glob(join(self.names[self.index], '*.png'))\n",
    "            clip = sorted(unsorted_clip, key=lambda x: float(x.split('/')[-1].split('.')[0]))\n",
    "            \n",
    "            image_skip_parameter = int(np.floor(len(clip)/self.max_num_images_per_clip))\n",
    "            for i in range(0,len(clip),image_skip_parameter):\n",
    "                x.append(self.image_loader(clip[i]))\n",
    "                if len(x) == self.max_num_images_per_clip:\n",
    "                    break\n",
    "            \n",
    "            target = torch.zeros(len(self.vocab), dtype=torch.float32)\n",
    "            \n",
    "            video_id = self.get_video_id(clip[0])\n",
    "            for word in self.most_freq_words[video_id]:\n",
    "                target[self.vocab.word2idx[word]] = 1\n",
    "              \n",
    "            self.index += 1\n",
    "            if self.index >= len(self.names):\n",
    "                self.index = 0\n",
    "            yield video_id, torch.stack(x).to(device), target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader()\n",
    "test_loader = DataLoader(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/image_captioning/model.py\n",
    "\n",
    "# class ResNetPreTrained(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "#         super(ResNetPreTrained, self).__init__()\n",
    "        \n",
    "#         resnet = models.resnet152(pretrained=False)\n",
    "#         modules = list(resnet.children())[:-1]  \n",
    "#         self.resnet = nn.Sequential(*modules)\n",
    "#         self.linear = nn.Linear(resnet.fc.in_features, 256)\n",
    "#         self.bn = nn.BatchNorm1d(256, momentum=0.01)\n",
    "        \n",
    "#     def forward(self, images):\n",
    "#         \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             features = self.resnet(images)\n",
    "#         return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning\n",
    "\n",
    "class WordAttributeGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, pre_trained):\n",
    "        super(WordAttributeGenerator, self).__init__()\n",
    "        \n",
    "        attr_net = models.resnet50(pretrained=pre_trained)\n",
    "        modules = list(attr_net.children())[:-1]      \n",
    "        self.attr_net = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(2048, 5000)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(5000, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        features = self.attr_net(images)\n",
    "        features = features.view(features.size()[0], -1)\n",
    "        weights = self.softmax(features)\n",
    "        features = torch.sum(weights * features, dim=0).view(1, -1)\n",
    "        features = self.tanh(self.fc1(features))\n",
    "        features = self.fc2(features)\n",
    "        return features.view(-1)\n",
    "    \n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.01)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning\n",
    "\n",
    "# class WordAttributeGenerator(nn.Module):\n",
    "#     def __init__(self, vocab_size, pre_trained):\n",
    "#         super(WordAttributeGenerator, self).__init__()\n",
    "        \n",
    "#         resnet_pretrained = ResNetPreTrained()\n",
    "#         resnet_pretrained.load_state_dict(torch.load(resnet_coco_weights))\n",
    "#         modules = list(resnet_pretrained.children())[:-2]\n",
    "#         self.resnet_pretrained = nn.Sequential(*modules)\n",
    "#         self.fc1 = nn.Linear(2048, 5000)\n",
    "#         self.tanh = nn.Tanh()\n",
    "#         self.fc2 = nn.Linear(5000, vocab_size)\n",
    "#         self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "#     def forward(self, images):\n",
    "#         \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "#         features = self.resnet_pretrained(images)\n",
    "#         features = features.view(features.size()[0], -1)\n",
    "#         weights = self.softmax(features)\n",
    "#         features = torch.sum(weights * features, dim=0).view(1, -1)\n",
    "#         features = self.tanh(self.fc1(features))\n",
    "#         features = self.fc2(features)\n",
    "#         return features.view(-1)\n",
    "    \n",
    "# def weights_init_normal(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Conv') != -1:\n",
    "#         torch.nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n",
    "#     elif classname.find('BatchNorm2d') != -1:\n",
    "#         torch.nn.init.normal_(m.weight.data, 1.0, 0.01)\n",
    "#         torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attr_net = WordAttributeGenerator(len(train_loader.get_vocab()), pre_trained=True).to(device)\n",
    "attr_net.train()\n",
    "attr_net.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(attr_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    with torch.no_grad():\n",
    "        vid, x, target = next(test_loader.data_generator())\n",
    "        features = attr_net(x)\n",
    "        loss = criterion(features, target)\n",
    "        \n",
    "    predicted = test_loader.get_words_from_index(torch.topk(features, num_most_common_words)[1])\n",
    "    actual = test_loader.get_words_from_index(torch.topk(target, num_most_common_words)[1])\n",
    "    print(\"Val Loss - {}, Video ID - {}, Output - {}, Target - {}\".format(loss.item(), vid, predicted, actual))\n",
    "    \n",
    "# attr_net.load_state_dict(torch.load(attr_net_weights))\n",
    "# attr_net.eval()\n",
    "# validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_interval = 25\n",
    "\n",
    "try:\n",
    "    attr_net.load_state_dict(torch.load(attr_net_weights))\n",
    "except:\n",
    "    print(\"The saved attr_net model file does not exist\")\n",
    "    \n",
    "for epoch in range(0, num_epochs):\n",
    "    for i in range(num_train_set):\n",
    "            \n",
    "        _, x, target = next(train_loader.data_generator())\n",
    "\n",
    "        optimizer.zero_grad();\n",
    "        features = attr_net(x)\n",
    "        loss = criterion(features, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics and save checkpoints\n",
    "        print(\"\\r[Epoch %d/%d] [Batch %d/%d] [Train loss: %f]\" %\n",
    "                                                        (epoch, num_epochs, i, num_train_set, loss.item()))\n",
    "\n",
    "        if i % sample_interval == 0:\n",
    "            torch.save(attr_net.state_dict(), attr_net_weights)\n",
    "            validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
